{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "#print (\"tensorflow v:\" + tf.__version__)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "#!conda install --yes --prefix {sys.prefix} opencv\n",
    "#!{sys.executable} -m pip install opencv-contrib-python\n",
    "#conda install -c conda-forge opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc\n",
      "skinny\n",
      "straight\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#images = []\n",
    "\n",
    "skinny_j = []\n",
    "straight_j = []\n",
    "bootc = []\n",
    "def find(obj, l, y):\n",
    "    dir1 = \"/Users/Simron/ds/examples/pants/proj/images/\" + obj\n",
    "   \n",
    "    \n",
    "    #for x in os.walk(dir1):\n",
    "            #print x\n",
    "    m_dir= glob.glob(dir1 + \"/*/*jpg\")\n",
    "    print obj\n",
    "    for files in m_dir:\n",
    "            image = cv2.imread(files, 1)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            #print image.shape\n",
    "            \n",
    "            #print image.shape\n",
    "            #asp = 100.0 / image.shape[1]\n",
    "            #dim = (100, int(image.shape[0]*asp))\n",
    "            \n",
    "            #resize = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "            #print resize.shape\n",
    "            #l.append(resize)\n",
    "            #tup = image.append(y)\n",
    "            l.append(image)\n",
    "            #print (\"shape:\", image.shape)\n",
    "    l = np.array(l)\n",
    "   \n",
    "    return l\n",
    "    \n",
    "bootc = find(\"bc\", bootc, 0)\n",
    "#print bootc[0].shape\n",
    "skinny_j = find(\"skinny\", skinny_j, 1)\n",
    "straight_j = find(\"straight\", straight_j, 2)\n",
    "\n",
    "y_bc = np.full((bootc.shape[0], 1), 0)\n",
    "y_skj = np.full((skinny_j.shape[0], 1), 1)\n",
    "y_sj = np.full((straight_j.shape[0], 1), 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the images come of different shapes and sizes so rather than downsampling certain images and risking the loss of data, we will resize and pad certain images and compare which is better. Converted images to grayscale because it took very long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#730 x 480\n",
    "\n",
    "def resize(l, size):\n",
    "    ret_l = []   \n",
    "    count = 0\n",
    "    for i in range(len(l)):        \n",
    "        o_size = l[i].shape[:2]\n",
    "\n",
    "        ratio = float(size)/max(o_size)\n",
    "        n_size = tuple([int(x*ratio) for x in o_size])\n",
    "\n",
    "        im = l[i]\n",
    "        #if (i == l.shape[0]-1):\n",
    "            #cv2.imshow(\"image1\", im)\n",
    "            #cv2.waitKey(0)\n",
    "            #cv2.destroyAllWindows()\n",
    "        im = cv2.resize(im, (n_size[1], n_size[0]))\n",
    "        #im = im.convert('L')\n",
    "        delta_w = size - n_size[1]\n",
    "        delta_h = size - n_size[0]\n",
    "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "        color = [0, 0, 0]\n",
    "        new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "        #print new_im.shape\n",
    "        #tup = (new_im, l[i])\n",
    "        #if (i == l.shape[0]-1):\n",
    "            #cv2.imshow(\"image2\", new_im)\n",
    "            #cv2.waitKey(0)\n",
    "            #cv2.destroyAllWindows()\n",
    "        ret_l.append(new_im)\n",
    "    #for e in np.array(ret_l):\n",
    "            #print e.shape\n",
    "    return np.array(ret_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootc_r = resize(bootc, 480)\n",
    "#print bootc_r[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skinny_r = resize(skinny_j, 480)\n",
    "straight_r = resize(straight_j, 480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = np.vstack([bootc_r, skinny_r, straight_r])\n",
    "y = np.vstack([y_bc, y_skj, y_sj])\n",
    "\n",
    "X2 = X\n",
    "y2 = y\n",
    "\n",
    "#print (\"x shape:\" + X.shape)\n",
    "#print (\"y shape:\" + y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall normalize the data by subtracting the mean of the samples from each sample. Additionally we will divide by the standard deviation of each pixel. Subtracting the mean centers the input to 0, and dividing by the standard deviation makes any scaled feature value the number of standard deviations away from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X2 = X - np.mean(X, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X2/np.std(X2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-27811cca64fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_t, y_tr, y_t = train_test_split(X2, y, test_size=0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#Convolution\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (480, 480, 1), activation = 'relu'))\n",
    "\n",
    "#Pooling\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Full connection\n",
    "model.add(Dense(units = 128, activation = 'relu'))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.resize(X_tr, (X_tr.shape[0], X_tr.shape[1], X_tr.shape[2], 1))\n",
    "#model.fit(X_tr, y_tr, shuffle=True, epochs=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, shuffle=True, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.resize(X_t, (X_t.shape[0], X_t.shape[1], X_t.shape[2], 1))\n",
    "score = model.evaluate(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "cv_scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
